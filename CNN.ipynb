{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ§  How a Convolutional Neural Network (CNN) Works\n",
        "\n",
        "CNN is a deep learning architecture designed to process grid-like data, especially images. Here's a breakdown of how a CNN works from start to finish:\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”¹ 1. Input Layer\n",
        "\n",
        "- The input is typically an image (e.g., grayscale 28Ã—28 pixels).\n",
        "- Each pixel is normalized:\n",
        "\n",
        "$$\n",
        "p_{\\text{normalized}} = \\frac{p}{255}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”¹ 2. Convolution Layer\n",
        "\n",
        "- A small matrix called a **kernel** or **filter** slides over the image.\n",
        "- At each position \\((i, j)\\), a dot product is calculated:\n",
        "\n",
        "$$\n",
        "S(i, j) = \\sum_{m=0}^{k-1} \\sum_{n=0}^{k-1} I(i+m, j+n) \\times K(m, n)\n",
        "$$\n",
        "\n",
        "\n",
        "- This operation detects features like edges, textures, etc.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”¹ 3. Activation Function (ReLU)\n",
        "\n",
        "- Applies a non-linear function:\n",
        "\n",
        "$$\n",
        "f(x) = \\max(0, x)\n",
        "$$\n",
        "\n",
        "- Negative values become zero; positive values remain the same.\n",
        "- Helps the network learn complex patterns.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”¹ 4. Pooling Layer (Max Pooling)\n",
        "\n",
        "- Reduces spatial size and keeps strong signals:\n",
        "\n",
        "$$\n",
        "P(i, j) = \\max \\{\n",
        "x_{2i, 2j},\\\n",
        "x_{2i+1, 2j},\\\n",
        "x_{2i, 2j+1},\\\n",
        "x_{2i+1, 2j+1}\n",
        "\\}\n",
        "$$\n",
        "\n",
        "- Reduces computation and prevents overfitting.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”¹ 5. Flattening\n",
        "\n",
        "- Converts the 2D pooled feature maps into a 1D vector:\n",
        "\n",
        "$$\n",
        "\\text{flattened} = [x_1, x_2, ..., x_n]\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”¹ 6. Fully Connected Layer (Dense)\n",
        "\n",
        "- Multiplies flattened input with weights and adds bias:\n",
        "\n",
        "$$\n",
        "\\text{logits} = W \\cdot \\text{flattened} + b\n",
        "$$\n",
        "\n",
        "- Produces raw class scores (logits).\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”¹ 7. Softmax Layer\n",
        "\n",
        "- Converts logits into probabilities:\n",
        "\n",
        "$$\n",
        "\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”¹ 8. Loss Function (Cross-Entropy)\n",
        "\n",
        "- Measures how well the prediction matches the true label:\n",
        "\n",
        "$$\n",
        "L = -\\log(p_{\\text{target}})\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”¹ 9. Backpropagation & Weight Update\n",
        "\n",
        "- Calculates gradient and updates weights:\n",
        "$$\n",
        "W := W - \\alpha \\frac{\\partial L}{\\partial W}, \\quad b := b - \\alpha \\frac{\\partial L}{\\partial b}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## âœ… Final Prediction\n",
        "\n",
        "- The class with the highest probability is chosen as output.\n",
        "\n",
        "---\n",
        "\n",
        "# âœ… Why Use CNN?\n",
        "\n",
        "- **Preserves spatial structure:** Unlike traditional ANNs, CNNs understand the layout and nearby relationships in images.\n",
        "- **Efficient with fewer parameters:** Thanks to local connectivity and weight sharing.\n",
        "- **Automatic feature extraction:** CNNs learn to detect edges, textures, shapes without manual intervention.\n",
        "- **Highly accurate in visual tasks:** Used in image classification, object detection, facial recognition, etc.\n",
        "\n",
        "---\n",
        "\n",
        "# ðŸ”„ Why (or When) Use ANN Instead?\n",
        "\n",
        "- **Use ANN when data is flat or tabular**, like:\n",
        "  - Customer records\n",
        "  - Stock market data\n",
        "  - Sensor values\n",
        "- **ANNs are simpler** and work well when the input doesn't have spatial/temporal structure.\n",
        "- **Not suitable for images or sequences** â€” unless combined with CNNs or RNNs.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "qJwM2UqJmQ3c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN Working: Simple Example with a Single Convolution Kernel\n",
        "\n",
        "This notebook demonstrates how a basic Convolutional Neural Network (CNN) works on a simple image using Python and NumPy.\n",
        "No advanced libraries or math are requiredâ€”just simple steps to understand the core ideas behind CNNs!\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Load and Prepare the Image\n",
        "\n",
        "We load an image, convert it to grayscale, resize it to 28x28 pixels, and normalize the pixel values to be between 0 and 1.\n"
      ],
      "metadata": {
        "id": "Y9zKYispeJij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# Load and prepare the image\n",
        "image = Image.open(\"/content/tree.jpg\").convert(\"L\")  # Grayscale conversion\n",
        "image = image.resize((28, 28))  # Resize image to 28x28\n",
        "input_image = np.array(image, dtype=float) / 255.0  # Normalize pixels to [0, 1]\n",
        "\n",
        "print(f\"Input Image shape: {input_image.shape}\")\n",
        "print(f\"Input Image sample data (5x5):\\n{input_image[:5, :5]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7VwFLfLKeJCV",
        "outputId": "afec2a65-0f6c-469c-86b2-be5a441801fc"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Image shape: (28, 28)\n",
            "Input Image sample data (5x5):\n",
            "[[0.2627451  0.26666667 0.2627451  0.2627451  0.26666667]\n",
            " [0.27058824 0.2745098  0.27843137 0.27843137 0.28627451]\n",
            " [0.29019608 0.29019608 0.29019608 0.29411765 0.29803922]\n",
            " [0.30588235 0.30588235 0.30196078 0.30588235 0.30980392]\n",
            " [0.31764706 0.31764706 0.31372549 0.31764706 0.32156863]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Description**:\n",
        "We start by loading a picture, turning it into black and white (grayscale), and resizing it to a small 28x28 pixel image. Then, the pixel colors are turned into numbers between 0 and 1 to make calculations easier. This small image will be the input for our CNN."
      ],
      "metadata": {
        "id": "o8hLMUf2f4io"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The image is converted to grayscale and resized to 28Ã—28 pixels.\n",
        "\n",
        "- Each pixel value \\( p \\) is normalized:\n",
        "\n",
        "$$\n",
        "p_{\\text{normalized}} = \\frac{p}{255}\n",
        "$$\n"
      ],
      "metadata": {
        "id": "J7lAhqTiiewu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Define Convolution Kernel\n",
        "A fixed 3x3 kernel to detect vertical edges in the image."
      ],
      "metadata": {
        "id": "R-V6NnCxeTXs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kernel = np.array([\n",
        "    [1, 0, -1],\n",
        "    [1, 0, -1],\n",
        "    [1, 0, -1]\n",
        "], dtype=float)\n",
        "\n",
        "print(f\"\\nConvolution Kernel:\\n{kernel}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UNihCJLFeI_A",
        "outputId": "cc48314f-f0c0-4e9a-95a5-0504a00d19a0"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Convolution Kernel:\n",
            "[[ 1.  0. -1.]\n",
            " [ 1.  0. -1.]\n",
            " [ 1.  0. -1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Description**:\n",
        "This small grid of numbers (called a kernel) acts like a filter. It will scan over the image to detect vertical edges â€” areas where color changes sharply from left to right.\n",
        "\n"
      ],
      "metadata": {
        "id": "N9w4j9Z-f98B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- This \\(3 \\times 3\\) matrix is a filter to detect vertical edges.\n",
        "\n",
        "- The kernel \\( K \\) looks like:\n",
        "\n",
        "$$\n",
        "K = \\begin{bmatrix}\n",
        "1 & 0 & -1 \\\\\n",
        "1 & 0 & -1 \\\\\n",
        "1 & 0 & -1\n",
        "\\end{bmatrix}\n",
        "$$\n"
      ],
      "metadata": {
        "id": "fKCdXPSYivVY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Define Convolution Operation\n",
        "This function performs a 2D convolution by sliding the kernel over the image and computing element-wise multiplication sums."
      ],
      "metadata": {
        "id": "vubygV0peY4P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convolve2d(img, kernel):\n",
        "    k = kernel.shape[0]  # Size of the kernel (3)\n",
        "    out_dim = img.shape[0] - k + 1  # Calculate output size after applying kernel\n",
        "    out = np.zeros((out_dim, out_dim))  # Prepare empty space for result\n",
        "    for i in range(out_dim):\n",
        "      for j in range(out_dim):\n",
        "          region = img[i:i+k, j:j+k]  # Grab a small part of the image\n",
        "          out[i, j] = np.sum(region * kernel)  # Multiply and add to get one number\n",
        "\n",
        "    print(f\"\\nAfter Convolution (shape {out.shape}): sample 5x5 values:\\n{out[:5, :5]}\")\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "evrCk_CmeW-w"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Description**:\n",
        "This function slides the kernel over the image, one small section at a time. It multiplies the kernel values with the image's pixel values and sums them up to create a new, smaller image highlighting edges.\n",
        "\n"
      ],
      "metadata": {
        "id": "m_fKCGwGgC41"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The kernel slides over the image.\n",
        "\n",
        "- For each position \\((i, j)\\), we compute:\n",
        "\n",
        "$$\n",
        "S(i, j) = \\sum_{m=0}^{k-1} \\sum_{n=0}^{k-1} I(i+m, j+n) \\times K(m, n)\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "- \\(I\\) is the input image  \n",
        "- \\(K\\) is the kernel  \n",
        "- \\(S\\) is the resulting feature map (output)\n"
      ],
      "metadata": {
        "id": "_pg9AL95i-Y-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Define ReLU Activation\n",
        "ReLU (Rectified Linear Unit) sets all negative values to zero, adding non-linearity."
      ],
      "metadata": {
        "id": "xlr6YPqJefy3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def relu(x):\n",
        "    activated = np.maximum(0, x)\n",
        "    print(f\"\\nAfter ReLU (shape {activated.shape}): sample 5x5 values:\\n{activated[:5, :5]}\")\n",
        "    return activated\n"
      ],
      "metadata": {
        "id": "E9ogEIpsecVz"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Description**:\n",
        "ReLU is a simple rule: if a number is negative, change it to zero; if it's positive, keep it. This helps the network focus on important features and ignore less useful informatio"
      ],
      "metadata": {
        "id": "0i45KuM1gGhI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Applies element-wise ReLU activation:\n",
        "\n",
        "$$\n",
        "f(x) = \\max(0, x)\n",
        "$$\n",
        "\n",
        "- Negative values become zero; positive values stay the same.\n"
      ],
      "metadata": {
        "id": "t2xX567yjN2k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Define Max Pooling\n",
        "This downsamples the feature map by taking the maximum value in non-overlapping 2x2 windows."
      ],
      "metadata": {
        "id": "x_rYhxSnensI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def max_pooling(x, size=2, stride=2):\n",
        "    h, w = x.shape\n",
        "    pooled = np.zeros((h//2, w//2))\n",
        "    for i in range(0, h, stride):\n",
        "        for j in range(0, w, stride):\n",
        "            pooled[i//2, j//2] = np.max(x[i:i+size, j:j+size])\n",
        "    print(f\"\\nAfter Max Pooling (shape {pooled.shape}): sample 5x5 values:\\n{pooled[:5, :5]}\")\n",
        "    return pooled\n"
      ],
      "metadata": {
        "id": "r8Md3miBekxB"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Description:**\n",
        "Max pooling shrinks the image by taking the largest value in small blocks. This reduces the image size while keeping the most important information, helping the network be faster and less sensitive to small changes."
      ],
      "metadata": {
        "id": "THZL9wXOgLkF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Max pooling reduces spatial size by taking the maximum value in each non-overlapping \\(2 \\times 2\\) block:\n",
        "\n",
        "$$\n",
        "P(i, j) = \\max \\{\n",
        "x_{2i, 2j},\\\n",
        "x_{2i+1, 2j},\\\n",
        "x_{2i, 2j+1},\\\n",
        "x_{2i+1, 2j+1}\n",
        "\\}\n",
        "$$\n",
        "\n",
        "- This reduces computation and keeps strong signals.\n"
      ],
      "metadata": {
        "id": "4DEq76VdjROr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Define Softmax Function\n",
        "Converts logits to probabilities summing to 1, for classification outputs."
      ],
      "metadata": {
        "id": "P9HmmLUkesRa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    sm = e_x / np.sum(e_x)\n",
        "    print(f\"\\nSoftmax probabilities: {sm}\")\n",
        "    return sm\n"
      ],
      "metadata": {
        "id": "ohjaG9cKeqVt"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Description:**\n",
        "Softmax turns a list of numbers into probabilities that add up to 100%. It helps us decide which class (e.g., \"tree\" or \"not tree\") the image most likely belongs to."
      ],
      "metadata": {
        "id": "w7LDqxkygPkV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Converts raw scores (logits) into probabilities:\n",
        "\n",
        "$$\n",
        "\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}}\n",
        "$$\n",
        "\n",
        "- Ensures probabilities sum to 1.\n"
      ],
      "metadata": {
        "id": "oq_Yo2bAjjw5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Define Cross-Entropy Loss\n",
        "Measures how far the predicted probabilities are from the true label."
      ],
      "metadata": {
        "id": "6yqBZvF3exm8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_entropy_loss(probs, target):\n",
        "    loss = -np.log(probs[target] + 1e-8)\n",
        "    print(f\"Cross-Entropy Loss: {loss}\")\n",
        "    return loss\n"
      ],
      "metadata": {
        "id": "0rEjwwttevgf"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Description:**\n",
        "This loss measures how far off our prediction is from the truth. The smaller the loss, the better the prediction."
      ],
      "metadata": {
        "id": "n5ujLWYPgS7U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Measures how well the predicted probability \\( p_{\\text{target}} \\) matches the true label:\n",
        "\n",
        "$$\n",
        "L = -\\log(p_{\\text{target}})\n",
        "$$\n",
        "\n",
        "- Lower loss means better prediction.\n"
      ],
      "metadata": {
        "id": "TRImuuy5jvbs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Derivative of Cross-Entropy Loss\n",
        "Needed for gradient calculation in backpropagation."
      ],
      "metadata": {
        "id": "UMJ5YH4Ce2nM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_entropy_derivative(probs, target):\n",
        "    grad = probs.copy()\n",
        "    grad[target] -= 1\n",
        "    return grad\n"
      ],
      "metadata": {
        "id": "b1ZB3G5Te05_"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Description:**\n",
        "This helps us figure out how to change the modelâ€™s settings (weights) to make the prediction better next time. Think of it as giving the model hints about what went wrong.\n",
        "\n"
      ],
      "metadata": {
        "id": "Fq4Ysw87gWID"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Computes gradient for adjusting model weights:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial z_i} = p_i - y_i\n",
        "$$\n",
        "\n",
        "- Where:\n",
        "  - \\( p_i \\) is the predicted probability  \n",
        "  - \\( y_i \\) is the true label (1 for correct class, 0 otherwise)\n"
      ],
      "metadata": {
        "id": "gXiBEEH6j8Gv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Initialize Training Parameters\n",
        "Set a dummy label and learning rate."
      ],
      "metadata": {
        "id": "uiU2tQ88e8nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label = 1  # Example label (e.g., class \"tree\")\n",
        "learning_rate = 0.1\n"
      ],
      "metadata": {
        "id": "WoUNauUMe6Sg"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Description:**\n",
        "Here we set the correct answer (label) for training and how quickly the model should learn (learning rate)."
      ],
      "metadata": {
        "id": "ZE43Hxo4gZsQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10. Initial Forward Pass to Determine Sizes\n",
        "Run through convolution, ReLU, and pooling once to determine the flattened feature vector size.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AUIxzzYWfBVH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conv_out = convolve2d(input_image, kernel)\n",
        "activated = relu(conv_out)\n",
        "pooled = max_pooling(activated)\n",
        "flattened = pooled.flatten()\n",
        "\n",
        "print(f\"\\nAfter Flattening: shape {flattened.shape}\")\n",
        "print(f\"Flattened vector sample (first 10 values): {flattened[:10]}\")\n",
        "\n",
        "fc_input_size = flattened.shape[0]\n",
        "\n",
        "# Initialize fully connected layer weights and biases\n",
        "np.random.seed(42)\n",
        "fc_weights = np.random.randn(2, fc_input_size) * 0.01  # 2 classes, small random weights\n",
        "fc_biases = np.zeros(2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMsvIrYUe-wF",
        "outputId": "6322a317-0ec5-41df-e627-6a10a57db0ae"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "After Convolution (shape (26, 26)): sample 5x5 values:\n",
            "[[-0.00784314 -0.00392157 -0.01960784 -0.02352941 -0.00392157]\n",
            " [-0.00392157 -0.00784314 -0.02352941 -0.01568627  0.00784314]\n",
            " [ 0.00784314 -0.00392157 -0.02352941 -0.01176471  0.00784314]\n",
            " [ 0.01176471  0.         -0.01568627 -0.00784314  0.00784314]\n",
            " [ 0.01176471  0.00784314 -0.00392157 -0.00392157  0.00392157]]\n",
            "\n",
            "After ReLU (shape (26, 26)): sample 5x5 values:\n",
            "[[0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.00784314]\n",
            " [0.00784314 0.         0.         0.         0.00784314]\n",
            " [0.01176471 0.         0.         0.         0.00784314]\n",
            " [0.01176471 0.00784314 0.         0.         0.00392157]]\n",
            "\n",
            "After Max Pooling (shape (13, 13)): sample 5x5 values:\n",
            "[[0.         0.         0.00784314 0.         0.00392157]\n",
            " [0.01176471 0.         0.00784314 0.         0.00784314]\n",
            " [0.01176471 0.00392157 0.00392157 0.         0.08235294]\n",
            " [0.01960784 0.00784314 0.         0.10980392 0.22352941]\n",
            " [0.03921569 0.02352941 0.         0.27058824 0.2627451 ]]\n",
            "\n",
            "After Flattening: shape (169,)\n",
            "Flattened vector sample (first 10 values): [0.00000000e+00 0.00000000e+00 7.84313725e-03 0.00000000e+00\n",
            " 3.92156863e-03 5.55111512e-17 3.92156863e-03 3.92156863e-03\n",
            " 3.92156863e-03 0.00000000e+00]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Description**:\n",
        "We run the image through the first steps to figure out how big the output will be. Then, we prepare a simple decision-making layer (fully connected layer) with small random starting settings."
      ],
      "metadata": {
        "id": "bXpyCft_gdQN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- After convolution, activation, and pooling, flatten the 2D data to a 1D vector:\n",
        "\n",
        "$$\n",
        "\\text{flattened} = [x_1, x_2, \\dots, x_n]\n",
        "$$\n",
        "\n",
        "- Initialize weights \\( W \\) and biases \\( b \\) for a fully connected layer with 2 outputs (classes):\n",
        "\n",
        "$$\n",
        "\\text{logits} = W \\cdot \\text{flattened} + b\n",
        "$$\n"
      ],
      "metadata": {
        "id": "TP1QXxF6kIOx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11. Training Loop (10 Epochs)\n",
        "Forward pass â†’ loss calculation â†’ backward pass â†’ parameter update."
      ],
      "metadata": {
        "id": "X9Bvn-cvfMIv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(5):\n",
        "    print(f\"\\n===== Epoch {epoch} =====\")\n",
        "\n",
        "    # Forward pass\n",
        "    conv_out = convolve2d(input_image, kernel)\n",
        "    activated = relu(conv_out)\n",
        "    pooled = max_pooling(activated)\n",
        "    flattened = pooled.flatten()\n",
        "    print(f\"Flattened input size: {flattened.shape}\")\n",
        "\n",
        "    logits = np.dot(fc_weights, flattened) + fc_biases\n",
        "    print(f\"Logits before softmax: {logits}\")\n",
        "\n",
        "    probs = softmax(logits)\n",
        "    loss = cross_entropy_loss(probs, label)\n",
        "\n",
        "    # Backward pass (gradient calculation)\n",
        "    dL_dlogits = cross_entropy_derivative(probs, label)\n",
        "    dL_dw = dL_dlogits[:, None] * flattened[None, :]  # Gradient for weights\n",
        "    dL_db = dL_dlogits  # Gradient for biases\n",
        "\n",
        "    # Update weights and biases using gradient descent\n",
        "    fc_weights -= learning_rate * dL_dw\n",
        "    fc_biases -= learning_rate * dL_db\n",
        "\n",
        "    pred_class = np.argmax(probs)\n",
        "    print(f\"Predicted class: {pred_class} | Correct: {pred_class == label}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpPYJrSbfFEW",
        "outputId": "f5d4bac5-b966-40b1-f730-88403664802b"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Epoch 0 =====\n",
            "\n",
            "After Convolution (shape (26, 26)): sample 5x5 values:\n",
            "[[-0.00784314 -0.00392157 -0.01960784 -0.02352941 -0.00392157]\n",
            " [-0.00392157 -0.00784314 -0.02352941 -0.01568627  0.00784314]\n",
            " [ 0.00784314 -0.00392157 -0.02352941 -0.01176471  0.00784314]\n",
            " [ 0.01176471  0.         -0.01568627 -0.00784314  0.00784314]\n",
            " [ 0.01176471  0.00784314 -0.00392157 -0.00392157  0.00392157]]\n",
            "\n",
            "After ReLU (shape (26, 26)): sample 5x5 values:\n",
            "[[0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.00784314]\n",
            " [0.00784314 0.         0.         0.         0.00784314]\n",
            " [0.01176471 0.         0.         0.         0.00784314]\n",
            " [0.01176471 0.00784314 0.         0.         0.00392157]]\n",
            "\n",
            "After Max Pooling (shape (13, 13)): sample 5x5 values:\n",
            "[[0.         0.         0.00784314 0.         0.00392157]\n",
            " [0.01176471 0.         0.00784314 0.         0.00784314]\n",
            " [0.01176471 0.00392157 0.00392157 0.         0.08235294]\n",
            " [0.01960784 0.00784314 0.         0.10980392 0.22352941]\n",
            " [0.03921569 0.02352941 0.         0.27058824 0.2627451 ]]\n",
            "Flattened input size: (169,)\n",
            "Logits before softmax: [ 0.00762963 -0.01700898]\n",
            "\n",
            "Softmax probabilities: [0.50615934 0.49384066]\n",
            "Cross-Entropy Loss: 0.7055423475309873\n",
            "Predicted class: 0 | Correct: False\n",
            "\n",
            "===== Epoch 1 =====\n",
            "\n",
            "After Convolution (shape (26, 26)): sample 5x5 values:\n",
            "[[-0.00784314 -0.00392157 -0.01960784 -0.02352941 -0.00392157]\n",
            " [-0.00392157 -0.00784314 -0.02352941 -0.01568627  0.00784314]\n",
            " [ 0.00784314 -0.00392157 -0.02352941 -0.01176471  0.00784314]\n",
            " [ 0.01176471  0.         -0.01568627 -0.00784314  0.00784314]\n",
            " [ 0.01176471  0.00784314 -0.00392157 -0.00392157  0.00392157]]\n",
            "\n",
            "After ReLU (shape (26, 26)): sample 5x5 values:\n",
            "[[0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.00784314]\n",
            " [0.00784314 0.         0.         0.         0.00784314]\n",
            " [0.01176471 0.         0.         0.         0.00784314]\n",
            " [0.01176471 0.00784314 0.         0.         0.00392157]]\n",
            "\n",
            "After Max Pooling (shape (13, 13)): sample 5x5 values:\n",
            "[[0.         0.         0.00784314 0.         0.00392157]\n",
            " [0.01176471 0.         0.00784314 0.         0.00784314]\n",
            " [0.01176471 0.00392157 0.00392157 0.         0.08235294]\n",
            " [0.01960784 0.00784314 0.         0.10980392 0.22352941]\n",
            " [0.03921569 0.02352941 0.         0.27058824 0.2627451 ]]\n",
            "Flattened input size: (169,)\n",
            "Logits before softmax: [-0.67293109  0.66355173]\n",
            "\n",
            "Softmax probabilities: [0.20808905 0.79191095]\n",
            "Cross-Entropy Loss: 0.23330632073729837\n",
            "Predicted class: 1 | Correct: True\n",
            "\n",
            "===== Epoch 2 =====\n",
            "\n",
            "After Convolution (shape (26, 26)): sample 5x5 values:\n",
            "[[-0.00784314 -0.00392157 -0.01960784 -0.02352941 -0.00392157]\n",
            " [-0.00392157 -0.00784314 -0.02352941 -0.01568627  0.00784314]\n",
            " [ 0.00784314 -0.00392157 -0.02352941 -0.01176471  0.00784314]\n",
            " [ 0.01176471  0.         -0.01568627 -0.00784314  0.00784314]\n",
            " [ 0.01176471  0.00784314 -0.00392157 -0.00392157  0.00392157]]\n",
            "\n",
            "After ReLU (shape (26, 26)): sample 5x5 values:\n",
            "[[0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.00784314]\n",
            " [0.00784314 0.         0.         0.         0.00784314]\n",
            " [0.01176471 0.         0.         0.         0.00784314]\n",
            " [0.01176471 0.00784314 0.         0.         0.00392157]]\n",
            "\n",
            "After Max Pooling (shape (13, 13)): sample 5x5 values:\n",
            "[[0.         0.         0.00784314 0.         0.00392157]\n",
            " [0.01176471 0.         0.00784314 0.         0.00784314]\n",
            " [0.01176471 0.00392157 0.00392157 0.         0.08235294]\n",
            " [0.01960784 0.00784314 0.         0.10980392 0.22352941]\n",
            " [0.03921569 0.02352941 0.         0.27058824 0.2627451 ]]\n",
            "Flattened input size: (169,)\n",
            "Logits before softmax: [-0.95271894  0.94333959]\n",
            "\n",
            "Softmax probabilities: [0.13055522 0.86944478]\n",
            "Cross-Entropy Loss: 0.13990044620625097\n",
            "Predicted class: 1 | Correct: True\n",
            "\n",
            "===== Epoch 3 =====\n",
            "\n",
            "After Convolution (shape (26, 26)): sample 5x5 values:\n",
            "[[-0.00784314 -0.00392157 -0.01960784 -0.02352941 -0.00392157]\n",
            " [-0.00392157 -0.00784314 -0.02352941 -0.01568627  0.00784314]\n",
            " [ 0.00784314 -0.00392157 -0.02352941 -0.01176471  0.00784314]\n",
            " [ 0.01176471  0.         -0.01568627 -0.00784314  0.00784314]\n",
            " [ 0.01176471  0.00784314 -0.00392157 -0.00392157  0.00392157]]\n",
            "\n",
            "After ReLU (shape (26, 26)): sample 5x5 values:\n",
            "[[0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.00784314]\n",
            " [0.00784314 0.         0.         0.         0.00784314]\n",
            " [0.01176471 0.         0.         0.         0.00784314]\n",
            " [0.01176471 0.00784314 0.         0.         0.00392157]]\n",
            "\n",
            "After Max Pooling (shape (13, 13)): sample 5x5 values:\n",
            "[[0.         0.         0.00784314 0.         0.00392157]\n",
            " [0.01176471 0.         0.00784314 0.         0.00784314]\n",
            " [0.01176471 0.00392157 0.00392157 0.         0.08235294]\n",
            " [0.01960784 0.00784314 0.         0.10980392 0.22352941]\n",
            " [0.03921569 0.02352941 0.         0.27058824 0.2627451 ]]\n",
            "Flattened input size: (169,)\n",
            "Logits before softmax: [-1.12825804  1.11887869]\n",
            "\n",
            "Softmax probabilities: [0.09559673 0.90440327]\n",
            "Cross-Entropy Loss: 0.10047991337781007\n",
            "Predicted class: 1 | Correct: True\n",
            "\n",
            "===== Epoch 4 =====\n",
            "\n",
            "After Convolution (shape (26, 26)): sample 5x5 values:\n",
            "[[-0.00784314 -0.00392157 -0.01960784 -0.02352941 -0.00392157]\n",
            " [-0.00392157 -0.00784314 -0.02352941 -0.01568627  0.00784314]\n",
            " [ 0.00784314 -0.00392157 -0.02352941 -0.01176471  0.00784314]\n",
            " [ 0.01176471  0.         -0.01568627 -0.00784314  0.00784314]\n",
            " [ 0.01176471  0.00784314 -0.00392157 -0.00392157  0.00392157]]\n",
            "\n",
            "After ReLU (shape (26, 26)): sample 5x5 values:\n",
            "[[0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.00784314]\n",
            " [0.00784314 0.         0.         0.         0.00784314]\n",
            " [0.01176471 0.         0.         0.         0.00784314]\n",
            " [0.01176471 0.00784314 0.         0.         0.00392157]]\n",
            "\n",
            "After Max Pooling (shape (13, 13)): sample 5x5 values:\n",
            "[[0.         0.         0.00784314 0.         0.00392157]\n",
            " [0.01176471 0.         0.00784314 0.         0.00784314]\n",
            " [0.01176471 0.00392157 0.00392157 0.         0.08235294]\n",
            " [0.01960784 0.00784314 0.         0.10980392 0.22352941]\n",
            " [0.03921569 0.02352941 0.         0.27058824 0.2627451 ]]\n",
            "Flattened input size: (169,)\n",
            "Logits before softmax: [-1.25679341  1.24741406]\n",
            "\n",
            "Softmax probabilities: [0.07556375 0.92443625]\n",
            "Cross-Entropy Loss: 0.07857117201867754\n",
            "Predicted class: 1 | Correct: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Description**:\n",
        "This is the main learning loop where the CNN makes predictions, measures mistakes, and adjusts itself to improve over 10 rounds (epochs). After each round, it tries to guess the correct class and improves gradually."
      ],
      "metadata": {
        "id": "Pl5IQc2vgg-K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- For each epoch (training step):\n",
        "\n",
        "  - Calculate convolution â†’ ReLU â†’ pooling â†’ flatten.\n",
        "\n",
        "  - Compute logits:\n",
        "\n",
        "    \n",
        "$$\n",
        "z = W \\cdot x + b\n",
        "$$\n",
        "    \n",
        "\n",
        "  - Compute softmax probabilities and loss.\n",
        "\n",
        "  - Calculate gradients and update weights/biases:\n",
        "\n",
        "   $$\n",
        "    W := W - \\alpha \\frac{\\partial L}{\\partial W}, \\quad b := b - \\alpha \\frac{\\partial L}{\\partial b}\n",
        "  $$\n",
        "\n",
        "    Where $\\( \\alpha \\)$ is the learning rate.\n",
        "\n",
        "  - Predict class as the one with the highest probability.\n"
      ],
      "metadata": {
        "id": "rRy14ekIkVKY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary\n",
        "This notebook shows a simplified version of how a CNN processes an image step-by-step:\n",
        "\n",
        "- It extracts edges with convolution.\n",
        "\n",
        "- Highlights important parts with ReLU.\n",
        "\n",
        "- Shrinks data with pooling.\n",
        "\n",
        "- Makes a prediction using a simple classifier.\n",
        "\n",
        "- Learns from mistakes through repeated training.\n",
        "\n",
        "This helps beginners understand the building blocks of CNNs in a clear and intuitive way"
      ],
      "metadata": {
        "id": "g0RXIOdlknqA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lAeU31yBfP_T"
      },
      "execution_count": 37,
      "outputs": []
    }
  ]
}